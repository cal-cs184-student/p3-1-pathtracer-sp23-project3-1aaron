<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: white;
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: #121212;
  }
</style>
<title>CS 184 Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
<h1 align="middle">Project 3-1: Path Tracer</h1>
<h2 align="middle">Aaron Shalf</h2>

<!-- Add Website URL -->
<h2 align="middle">Website URL: <a href="TODO">TODO</a></h2>

<br><br>


<div align="center">
  <table style="width=100%">
      <tr>
          <td align="middle">
          <img src="images/example_image.png" width="480px" />
          <figcaption align="middle">Results Caption: my bunny is the bounciest bunny</figcaption>
      </tr>
  </table>
</div>

<h2 align="middle">Overview</h2>
<p>
    Over the course of the project, I implemented a ray tracing algorithm that accounts for both direct and indirect illumination, which also involves some optimizations
  such as bounding volumes and adaptive lighting. The simplest and most basic ray tracing just involves determining if any rays travel directly from the light sources to
  the camera. Translating from the world space to the image space mainly involved coordinate space transformations, which allowed the casting of rays from the camera into
  the world. Once we could cast rays, we could recursively bounce them around the scene to accumulate reflective lighting. Algorithms such as adaptive lighting and Russian
  Roulette were implemented to make sure no infinite recursion or unnecessary bounces occurred. Furthermore, scenes were initialized into bounding volumes to further reduce
  the number of intersections that needed to be calculated. Together, these enabled the rendering of scenes in a manner fairly similar to real-world behavior.
</p>
<br>

<h2 align="middle">Part 1: Ray Generation and Scene Intersection (20 Points)</h2>
<!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files. -->

<h3>
  Walk through the ray generation and primitive intersection parts of the rendering pipeline.
</h3>
<p>
    Ray generation involved converting image coordinates into camera space coordinates by projecting them into a plane with a z-value of -1.
  A ray was then generated using that new point and the camera origin, which was then converted to the world space using a pre-provided coordinate
  matrix. Once it was possible to generate camera rays, these rays could be tested against primitive objects in the world using geometric formulas.
</p>
<br>

<h3>
  Explain the triangle intersection algorithm you implemented in your own words.
</h3>
<p>
    The triangle intersections were calculated by first generating barycentric coordinates for the ray's intersection with the triangle plane using
  the Moller Trumbore algorithm. Once the barycentric coordinates were produced, checking if the intersection lied within the triangle just depended on
  making sure each barycentric coordinate was between 0 and 1. If the time of the intersection was between min_time and max_time and the intersection
  point was within the triangle, then the algorithm could return true.
</p>
<br>

<h3>
  Show images with normal shading for a few small .dae files.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/CBspheres.png" align="middle" width="400px"/>
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </td>
      <td>
        <img src="images/CBgems.png" align="middle" width="400px"/>
        <figcaption>CBgems.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/CBcoil.png" align="middle" width="400px"/>
        <figcaption>CBcoil.dae</figcaption>
      </td>
      <td>
        <img src="images/cow.png" align="middle" width="400px"/>
        <figcaption>cow.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


<h2 align="middle">Part 2: Bounding Volume Hierarchy (20 Points)</h2>
<!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

<h3>
  Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
</h3>
<p>
  My BVH was constructed by creating a bounding box out of all primitives in the node, then determining whether or not the node should become a leaf. If there were over
  max_leaf_size primitives, the node would become an inner node and I'd sort all the primitives along an axis then split them into left/right based on my heuristic. If it
  was a leaf node, then the primitives just needed to be set in node.start and node.end.
    I chose the splitting point as the mean of the centroids of all primitives in the shape. Although this might be biased by outlier primitives,
  It is simplest to implement, as we already iterate through all of the primitives to generate the bounding box. For the axis, I chose whichever axis had the larger
  extent, or which dimension of the box was longest. This was to make sure the boxes were relatively compact.
</p>

<h3>
  Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/max.png" align="middle" width="400px"/>
        <figcaption>maxplanck.dae</figcaption>
      </td>
      <td>
        <img src="images/CBLucy.png" align="middle" width="400px"/>
        <figcaption>CBLucy.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/blob.png" align="middle" width="400px"/>
        <figcaption>blob.dae</figcaption>
      </td>
      <td>
        <img src="images/bench.png" align="middle" width="400px"/>
        <figcaption>bench.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.
</h3>
<p>
    Without BVH acceleration, many moderately complex to complex geometries would take upwards of several minutes to complete. However after applying BVH acceleration, no single file takes more than a minute
    to render, in fact the highest time seems to be the space of several seconds. This is due to the fact that the algorithmic complexity has been reduced from O(n) to O(log(n)). Since logarithmic runtimes asymptote
    at some time value, this is expected behavior. Simple files that would've taken a minute to render take less than a second, and complex ones that would've taken several now take no more than a couple seconds.
</p>
<br>

<h2 align="middle">Part 3: Direct Illumination (20 Points)</h2>
<!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

<h3>
  Walk through both implementations of the direct lighting function.
</h3>
<p>
    There are two approaches to direct lighting: uniform hemisphere sampling, and light importance sampling. Uniform hemipshere sampling uses a pdf with equal probability across all points on the hemisphere. By first
  generating a randomly sampled vector from this hemisphere and then checking if the ray from that vector intersects any light sources, we can simulate a bounce from that light source to the surface and back to the camera.

  In light importance sampling, we instead sample over the light sources and check if a ray can travel uninterrupted between the light and the surface. Using the rays and light values, we can calculate the radiance on
  the surface produced by each light source and average over the light sources.
</p>

<h3>
  Show some images rendered with both implementations of the direct lighting function.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <!-- Header -->
    <tr align="center">
      <th>
        <b>Uniform Hemisphere Sampling</b>
      </th>
      <th>
        <b>Light Sampling</b>
      </th>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/spheres_64_32.png" align="middle" width="400px"/>
        <figcaption>spheres.dae (hemisphere)</figcaption>
      </td>
      <td>
        <img src="images/CBbunny_H_64_32.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae (hemisphere)</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/spheres_no_H_64_32.png" align="middle" width="400px"/>
        <figcaption>spheres.dae (light importance)</figcaption>
      </td>
      <td>
        <img src="images/bunny_64_32.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae (light importance)</figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>
<br>

<h3>
  Focus on one particular scene with at least one area light and compare the noise levels in <b>soft shadows</b> when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, <b>not</b> uniform hemisphere sampling.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/bunny_1_32.png" align="middle" width="200px"/>
        <figcaption>1 Light Ray (example1.dae)</figcaption>
      </td>
      <td>
        <img src="images/bunny_4_32.png" align="middle" width="200px"/>
        <figcaption>4 Light Rays (example1.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/bunny_16_32.png" align="middle" width="200px"/>
        <figcaption>16 Light Rays (example1.dae)</figcaption>
      </td>
      <td>
        <img src="images/bunny_64_32.png" align="middle" width="200px"/>
        <figcaption>64 Light Rays (example1.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
    As the number of samples increases, the surface reflectance becomes less sensitive to variance with regard to the distance from the sampled light source. Since we
  take multiple samples for non-point light sources, the randomness comes from the differences in locations along the light source. With lower sampling rates, we increase
  the chances of accidentally sampling vectors that aren't representative of the lighting of the point, for instance we might sample a shaded point when most of the light
  actually does reach the point. As the samples increase, we end up accounting more for all of these vectors, which results in more of a shade gradient.
</p>
<br>

<h3>
  Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.
</h3>
<p>
    Uniform hemisphere sampling results in much more noise, as it weights samples from every location across the hemisphere evenly. Furthermore, it's not feasible
  to apply uniform hemisphere sampling to environments with point lights, as the sampled rays will never point directly to the light. Compared to lighting sampling, there are
  more noisy light points in the environment. Light sampling produces a much more consistent image, to a conspicuous degree. For instance, the ceiling is always pitch black
  in a single bounce light sampled image, because the rays between that surface and the light are always considered shadowed. Overall, light sampling is smoother and
  more consistent with the geometries, but has a somewhat overproduced and artificial look to it.
</p>
<br>


<h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
<!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

<h3>
  Walk through your implementation of the indirect lighting function.
</h3>
<p>
    The indirect lighting function makes use of the direct lighting functions to trace rays and determine the overall illuminance. The primary difference is the recursion. The base cases, where the max ray depth is 1 or 0, result
  in one_bounce or zero_bounce lighting respectively. Else-wise, we determine the one bounce radiance, then trace a new ray to another random direction and recursively determine the radiance contributed by that direction. Since this
  is probabilistic, we weight the new contributing rays by their probability and determine the radiance using sample_f.
</p>
<br>

<h3>
  Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/spheres1024.png" align="middle" width="400px"/>
        <figcaption>spheres.dae</figcaption>
      </td>
      <td>
        <img src="images/bunny1024_d3.png" align="middle" width="400px"/>
        <figcaption>bunny.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/spheres1024d.png" align="middle" width="400px"/>
        <figcaption>Only direct illumination (spheres.dae)</figcaption>
      </td>
      <td>
        <img src="images/spheres1024only_ind.png" align="middle" width="400px"/>
        <figcaption>Only indirect illumination (spheres.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    Direct illumination just uses the parts determined by Part 3, or one bounce and zero bounce illuminance. This results in only the rays directly cast to the surfaces from the light contributing
  to the resulting illuminance, and therefore the shaded regions receive no light and are completely dark. The ceiling also doesn't receive any lighting, as the rays aren't allowed to reflect back to it.

  In the direct lighting case, only the contributions made from >=1 bounces are incorporated, resulting in much more distributed lighting (as all reflection angles can be accounted for). Since none of the
  rays are weighted as heavily as the one/zero bounce rays, there are slightly less defined shapes and borders.
</p>
<br>

<h3>
  For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/bunny1024_d0.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/bunny1024_d1.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/bunny1024_d2.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/bunny1024_d3.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/bunny1024_d100.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    Max ray depths of 0 and 1 result in zero bounce and one bounce illumination respectively. Further than that provides more ambient lighting, which results in more color reflections and less
  strict shadowing. Although 2 bounces and 3 bounces result in visible differences, the difference between 3 bounces and 100 isn't linear with the max depth size, as russian roulette results in
  most rays terminating before they reach 100 bounces, which wouldn't contribute a relatively large amount of light anyways.
</p>
<br>

<h3>
  Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/spheres1_4.png" align="middle" width="400px"/>
        <figcaption>1 sample per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/spheres2_4.png" align="middle" width="400px"/>
        <figcaption>2 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/spheres4_4.png" align="middle" width="400px"/>
        <figcaption>4 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/spheres8_4.png" align="middle" width="400px"/>
        <figcaption>8 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/spheres16_4.png" align="middle" width="400px"/>
        <figcaption>16 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/spheres64_4.png" align="middle" width="400px"/>
        <figcaption>64 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/spheres1024_4.png" align="middle" width="400px"/>
        <figcaption>1024 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    Once again, the sample-per-pixel rates control how much variance is experienced in the ray tracing. For low amounts of samples,
  there is much more variance in the brightness of each pixel regardless of its location, as the rays can bounce in any direction. This results in
  speckles and inconsistent patterns on the surfaces of the spheres. In contrast, increasing the sampling results in the pixel values converging more to their
  expected values, which provides a smoother gradient. Visually, this seems to smooth out at around 64 samples per pixel, but only truly disappears at 1024 samples.
</p>
<br>


<h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
<!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

<h3>
  Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
</h3>
<p>
    Adaptative sampling is something of an optimization for ray tracing. By introducing a termination condition for our sampling, we can increase the overall number of samples we take. The goal is to terminate early
  for locations whose illuminance converges quickly. For areas that don't converge, we can dedicate more samples to it, resulting in less noisy results. Since the determining factor for terminating is the variance of the samples,
  we ensure that the variance is under some threshold for each point, we can be 95% confident that the illuminance we assign is correct, unless we reach our upper threshold, which still would involve enough samples to substantially
  reduce noise. To implement this, I followed the formulas for I, the standard deviation, and the mean, and checked the condition once every sampleBatchSize iterations of the raytracing loop.
</p>
<br>

<h3>
  Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/bunny_adaptive.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/bunny_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/spheres_adaptive.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/spheres_adaptive_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


</body>
</html>
